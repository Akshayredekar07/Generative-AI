{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED    \n",
      "deepseek-r1:1.5b           e0979632db5a    1.1 GB    2 weeks ago    \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    3 weeks ago    \n",
      "llama3.2:1b                baf6a787fdff    1.3 GB    3 weeks ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do cows wear bells?\n",
      "\n",
      "Because their horns don't work!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about {content}.\", \n",
    "    input_variables=[\"adjective\", \"content\"])\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "print(llm.invoke(prompt.format(adjective=\"funny\", content=\"cows\")).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the cow join a band?\n",
      "\n",
      "Because she wanted to be a moo-sician!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about {content}.\", \n",
    "    input_variables=[\"adjective\", \"content\"])\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "print(llm.invoke(prompt.format(adjective=\"funny\", content=\"cows\")).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot prompt \n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Summarize {text} in one sentence.\", \n",
    "    input_variables=[\"text\"])\n",
    "\n",
    "print(llm.invoke(prompt.format(text=\"Mumbai is the financial capital of India.\")).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mumbai, also known as Bombay, is the financial capital of India and a major hub for business, finance, and industry, serving as a global center for trade, commerce, and economic growth.\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Summarize {text} in one sentence.\",\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "\n",
    "final_prompt = prompt.format(\n",
    "    text=\"Mumbai is the financial capital of india.\"\n",
    ")\n",
    "\n",
    "result = llm.invoke(final_prompt)\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Bonjour, merci.\n"
     ]
    }
   ],
   "source": [
    "# Few-Shot Prompt Template\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"Hello\", \"output\": \"bonjour\"},\n",
    "    {\"input\": \"Good Morning\", \"output\": \"Bon matin\"}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    template=\"{input} translates to {output} in French.\",\n",
    "    input_variables=[\"input\", \"output\"]\n",
    ")\n",
    "\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Translate the follwing to French: \\n\",\n",
    "    suffix=\"Input: {input}\\nOutput:\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "final_prompt = few_shot_prompt.format(input=\"Thank you\")\n",
    "\n",
    "output = llm.invoke(final_prompt)\n",
    "\n",
    "print(type(output))\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Bonsoir, c'est mon Bonjour.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Namaste\"\n",
    "final_prompt = f\"\"\"\n",
    "Translate the following to French:\n",
    "\n",
    "Hello translates to bonjour in French.\n",
    "Good Morning translates to Bon matin in French.\n",
    "\n",
    "Input: {input_text}\n",
    "Output:\n",
    "\"\"\"\n",
    "output = llm.invoke(final_prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour ! L'expression \"Good Morning\" peut √™tre traduite en \"Bonne matin√©e\" en fran√ßais. \n",
      "\n",
      "Cependant, il est important de noter que les expressions d'horaire sont g√©n√©ralement traduites avec un adjectif qui correspond √† la bonne heure, ce qui n'est pas le cas ici. \n",
      "\n",
      "Alternativement, vous pouvez utiliser l'expression \"Bonne matin√©e\" ou simplement \"Matin\" pour une r√©ponse plus concise.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Translate the following to French:\n",
    "Hello translates to bonjour in French.\n",
    "Good Morning translates to Bon matin in French.\n",
    "Input: {input}\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input\"])\n",
    "input_text = \"\"\"Hi,How can I assist you today?\"\"\"\n",
    "# output: Bonjour, Comment puis-je vous aider aujourd'hui ?\n",
    "\n",
    "final_prompt = prompt.format(input=input)\n",
    "\n",
    "output = llm.invoke(final_prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je suis d√©sol√©, mais \"I\" est un pronoms personnel et n'est pas traditionnellement utilis√© en fran√ßais pour demander des services ou une assistance.\n",
      "\n",
      "Une alternative pourrait √™tre : Comment puis-je vous aider aujourd'hui ?\n",
      "\n",
      "Ce choix de phrase est plus appropri√© pour demander du soutien ou d'aide.\n"
     ]
    }
   ],
   "source": [
    "# ChatPromptTemplate (More Modern Approach)\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"Hello\", \"output\": \"bonjour\"},\n",
    "    {\"input\": \"Good Morning\", \"output\": \"Bon matin\"}\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_template(\"{input} translates to {output} in French.\")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Translate the following to French:\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"Input: {input}\\nOutput:\"),\n",
    "])\n",
    "\n",
    "output = llm.invoke(final_prompt.format_messages(input=\"How can I assist you today?\"))\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Chain of Thought (CoT) Prompt Template\n",
    "\n",
    "prompt = PromptTemplate(template=\"solve {question} step by step.\",\n",
    "                        input_variables=[\"question\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Modern ChatPromptTemplate approach for Chain of Thought\n",
    "\n",
    "cot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that solves problems step by step.\"),\n",
    "    (\"human\", \"Solve {question} step by step. Show your reasoning for each step.\")\n",
    "])\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3:4b\")\n",
    "\n",
    "output = llm.invoke(cot_prompt.format_messages(question=\"What is 15% of 200?\"))\n",
    "print(output.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is **New Delhi**. \n",
      "\n",
      "It‚Äôs a fascinating city with a rich history and a blend of old and new! üòä \n",
      "\n",
      "Do you want to know anything more about New Delhi or perhaps India in general?\n"
     ]
    }
   ],
   "source": [
    "# 5. Role-Based Prompt Template\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"What is the capital of {country}?\")\n",
    "])\n",
    "\n",
    "output = llm.invoke(prompt.format_messages(country=\"India\"))\n",
    "\n",
    "print(output.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I‚Äôm doing well, thank you for asking! As a large language model, I don't really *feel* in the way humans do, but my systems are running smoothly and I‚Äôm ready to chat. üòä \n",
      "\n",
      "How about you? How‚Äôs your day going so far?\n"
     ]
    }
   ],
   "source": [
    "# 6. Multi-Turn Prompt Template \n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Hello\"),\n",
    "    (\"assistant\", \"Hi! How can I help you today?\"),\n",
    "    (\"user\", \"{new_input}\")\n",
    "])\n",
    "\n",
    "final_prompt = prompt.format_messages(new_input=\"How are you?\")\n",
    "\n",
    "output = llm.invoke(final_prompt)\n",
    "\n",
    "print(output.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi\n"
     ]
    }
   ],
   "source": [
    "# 7. Question-Answering with FAISS + Embeddings \n",
    "\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from pydantic import SecretStr\n",
    "\n",
    "docs = [Document(page_content=\"Delhi is the capital city of India.\"),\n",
    "        Document(page_content=\"Mumbai is the financial capital of India.\")]\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-001\", \n",
    "    google_api_key=SecretStr(os.getenv(\"GOOGLE_API_KEY\") or \"\")\n",
    ")\n",
    "\n",
    "\n",
    "vs = FAISS.from_documents(docs, embeddings)\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Context: {context}\\nQuestion: {input}\\nAnswer:\")\n",
    "\n",
    "doc_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, doc_chain)\n",
    "\n",
    "output = retrieval_chain.invoke({\"input\": \"What is the capital of India?\"})[\"answer\"]\n",
    "\n",
    "print(output)\n",
    "\n",
    "\n",
    "\n",
    "# Program Flow:\n",
    "# 1. Import libraries for LangChain, FAISS, and Google embeddings\n",
    "# 2. Create two Document objects with text about Indian cities\n",
    "# 3. Initialize Google embeddings model with API key\n",
    "# 4. Embed documents into vectors and store in FAISS vector store\n",
    "# 5. Set up retriever to fetch top 1 document based on similarity\n",
    "# 6. Define a prompt template for question-answering with context\n",
    "# 7. Create a document chain to combine prompt and language model\n",
    "# 8. Build a retrieval chain linking retriever and document chain\n",
    "# 9. Invoke chain with query \"What is the capital of India?\"\n",
    "# 10. Retrieve closest document, format prompt, get llm response\n",
    "# 11. Print the answer, e.g., \"Delhi is the capital city of India.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. \n",
      "\n",
      "Do you want to know anything more about Paris? üòä\n"
     ]
    }
   ],
   "source": [
    "# 8. Dynamic Prompt Template\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3:4b\")\n",
    "\n",
    "user_input = \"Capital of France?\"\n",
    "prompt = PromptTemplate(template=f\"Answer {user_input}\", input_variables=[])\n",
    "print(llm.invoke(prompt.format()).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common and natural translation of \"Thank you\" to French is:\n",
      "\n",
      "**Merci** \n",
      "\n",
      "You can also say:\n",
      "\n",
      "*   **Merci beaucoup** (Thank you very much)\n",
      "\n",
      "Would you like me to elaborate on the nuances of saying \"thank you\" in French?\n"
     ]
    }
   ],
   "source": [
    "# 9. Instruction Tuning Prompt Template\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Instruction: Translate to French, Response: {text}\", \n",
    "    input_variables=[\"text\"])\n",
    "\n",
    "print(llm.invoke(prompt.format(text=\"Thank you\")).content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Function Calling Prompt Template \n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     template=\"Use def get_time(): return current time to answer {query}.\", \n",
    "#     input_variables=[\"query\"])\n",
    "\n",
    "# print(llm.invoke(prompt.format(query=\"What time is it?\")).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear AI,\n",
      "\n",
      "I‚Äôm doing well, thank you for asking! It‚Äôs interesting to hear from you. How are *you* doing today?\n",
      "\n",
      "Regards,\n",
      "John\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11. Prefix and Suffix Prompt Template\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Dear {name}, how are you? Regards, AI\", \n",
    "    input_variables=[\"name\"])\n",
    "\n",
    "print(llm.invoke(prompt.format(name=\"John\")).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Conditional Prompt Template\n",
    "\n",
    "user_is_premium = True\n",
    "\n",
    "template = \"Premium content: {content}\" if user_is_premium else \"Basic content: {content}\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"content\"])\n",
    "\n",
    "print(llm.invoke(prompt.format(content=\"Exclusive data\")).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock LLM response: Bonjour, John!\n"
     ]
    }
   ],
   "source": [
    "# Internationalization Template\n",
    "lang = \"fr\"\n",
    "greetings = {\"en\": \"Hello\", \"fr\": \"Bonjour\"}\n",
    "prompt = PromptTemplate(template=\"{greeting}, {name}!\", input_variables=[\"greeting\", \"name\"])\n",
    "result = llm.invoke(prompt.format(greeting=greetings[lang], name=\"John\")).content\n",
    "print(result)\n",
    "# Output: Mock LLM response: Bonjour, John!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide me with the description of the cat! I need you to tell me what the image looks like in order for me to respond. üòä \n",
      "\n",
      "For example, you could tell me:\n",
      "\n",
      "* \"The image shows a fluffy ginger tabby cat sleeping on a blue blanket.\"\n",
      "* \"The image is a close-up of a black cat with bright green eyes.\"\n",
      "* \"It's a photo of a Siamese cat stretching.\"\n",
      "\n",
      "\n",
      "Once you give me the description, I'll do my best to respond and engage with you about the image!\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Image: [description of {object}]\", \n",
    "    input_variables=[\"object\"])\n",
    "\n",
    "print(llm.invoke(prompt.format(object=\"cat\")).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I‚Äôve analyzed the image of ‚Äúcat.png‚Äù. Here's a description of the breed and mood:\n",
      "\n",
      "This appears to be a Maine Coon cat, characterized by its large size, fluffy coat, and bushy tail. The cat has a relaxed and contented mood, with a gentle expression. Its fur looks soft and inviting, suggesting a calm and friendly personality. The overall impression is one of serene beauty and gentle curiosity.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = Ollama(model=\"gemma3:4b\")\n",
    "template = \"\"\"\n",
    "You are a helpful assistant that analyzes images.\n",
    "Image path: {image_path}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "response = chain.run({\n",
    "    # \"image_path\": \"image.png\",  # Ollama internally reads this\n",
    "    \"image_path\": \"cat.png\", \n",
    "    \"question\": \"Describe the breed and mood of this animal write the 4 to 5 line description about it\"\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
